{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.  Can we use Bagging for regression problems ?\n",
        "   - Yes, Bagging (Bootstrap Aggregating) can absolutely be used for regression problems. While it's often discussed in the context of classification (e.g., Random Forests for classification), the core principle of aggregating predictions from multiple models built on bootstrapped samples is equally applicable to regression tasks. For regression, instead of taking a majority vote (as in classification), the predictions from the individual models are typically averaged to produce the final output.\n",
        "\n"
      ],
      "metadata": {
        "id": "mADtxweMb7eC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. What is the difference between multiple model training and single model training ?\n",
        "   - Single model training involves training one model on a dataset to perform a specific task. Its performance is limited by the model's capacity and the data it's trained on.\n",
        "\n",
        "- Multiple model training, often referred to as ensemble learning, involves training several models (either of the same type or different types) and then combining their predictions. This approach can lead to improved performance, robustness, and generalization compared to a single model . common techniques include  - Bagging, Boosting , Stacking"
      ],
      "metadata": {
        "id": "9k48n-T9b7al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  Explain the concept of feature randomness in Random Forest\n",
        "  - Feature randomness is a technique used in Random Forests where each decision tree is allowed to consider only a random subset of features at every split, instead of using all the features in the dataset.\n",
        "\n",
        "- This means that when a tree tries to decide the best feature to split on, it does not evaluate all features. Instead, it evaluates only a randomly selected group of features. The best splitting feature is chosen from this smaller random subset."
      ],
      "metadata": {
        "id": "MuvevOUPb7G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.  What is OOB (Out-of-Bag) Score ?\n",
        "- The Out-of-Bag (OOB) score is an internal method of evaluating the performance of a Random Forest without using a separate validation or test set.\n",
        "When building each tree in a Random Forest, the algorithm uses bootstrap sampling (sampling with replacement).\n",
        "Because of this, about 63% of the data is used to train a tree, and the remaining 37% samples are not selected.\n",
        "These unused samples are called Out-of-Bag samples.\n"
      ],
      "metadata": {
        "id": "z2VzjdInb7DU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. How can you measure the importance of features in a Random Forest model ?\n",
        "  - Random Forest provides two main ways to measure how important each feature is in making predictions:\n",
        "\n",
        "   1. Gini Importance (Mean Decrease in Impurity – MDI)-This is the default feature importance method in most Random Forest implementations.\n",
        "   - How it works  \n",
        "Every time a feature is used to split a node in a tree, it reduces impurity (like Gini impurity or entropy).\n",
        "The amount of impurity reduction is recorded.\n",
        "The reductions from all trees are averaged for each feature.\n",
        "\n",
        " - A feature with a higher impurity decrease is considered more important.\n",
        "\n",
        "2. Permutation Importance (Mean Decrease in Accuracy – MDA)-This method measures how much the model’s accuracy drops when the values of a feature are randomly shuffled.\n",
        "\n",
        "- How it works:\n",
        "Shuffle (permute) one feature’s values while keeping others the same.\n",
        "Measure how much model accuracy decreases.\n",
        "A larger drop means the feature was important.\n",
        "- If accuracy drops a lot → the feature has high importance.\n",
        "If accuracy barely changes → the feature is not important."
      ],
      "metadata": {
        "id": "gnxqJReqeBZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.  Explain the working principle of a Bagging Classifier\n",
        "- The Bagging Classifier (Bootstrap Aggregating Classifier) is an ensemble machine learning algorithm designed to improve the stability and accuracy of classification algorithms, often used to reduce variance and help avoid overfitting. It works by combining predictions from multiple decision trees (or other base learners) to produce a more robust final prediction.\n",
        " - The working principle includes the following steps:\n",
        "\n",
        "1. Bootstrap Sampling (Sampling With Replacement)-\n",
        "From the original dataset, the algorithm creates multiple new training sets by randomly selecting samples with replacement.\n",
        "Each new dataset is called a bootstrap sample and is usually the same size as the original set.\n",
        "This means some samples are repeated, and some are left out (called Out-of-Bag samples).\n",
        "\n",
        "2. Train Multiple Base Models-For each bootstrap sample, a separate model (usually a decision tree) is trained.\n",
        "Since each model sees slightly different training data, they learn different patterns.\n",
        "This reduces overfitting and increases diversity.\n",
        "\n",
        "3. Aggregate the Predictions-\n",
        "Once all models are trained, their predictions are combined:\n",
        "For classification → use majority voting\n",
        "(the class predicted by most models is the final output)\n",
        "For regression → use averaging\n",
        "(take the mean of predictions from all models)\n",
        "\n",
        "4. Final Decision\n",
        "The aggregated output becomes the final prediction of the Bagging Classifier."
      ],
      "metadata": {
        "id": "keCmz4WjfhFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.  How do you evaluate a Bagging Classifier's performance ?\n",
        "- The performance of a Bagging Classifier is usually evaluated by testing it on unseen data and checking how accurately it makes predictions. You can split the dataset into training and testing parts, train the model on the training set, and then measure accuracy, precision, recall, or F1-score on the test set. Another common method is to use the Out-of-Bag (OOB) score, where the model evaluates itself using the samples that were not included in the bootstrap training sets. This gives a reliable estimate of how well the Bagging Classifier will perform on new data.\n"
      ],
      "metadata": {
        "id": "5Mu8-NzXgoJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.  How does a Bagging Regressor work ?\n",
        "  - A Bagging Regressor works by creating several different training datasets using bootstrap sampling and then training a separate regression model on each of these datasets. Because each model sees slightly different data, they learn different patterns. When making a prediction, the Bagging Regressor takes the average of predictions from all the individual models. This averaging reduces variance, makes the model more stable, and improves its ability to generalize to new data."
      ],
      "metadata": {
        "id": "jkrobzoRhK9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. What is the main advantage of ensemble techniques ?\n",
        "  - The main advantage of ensemble techniques is that they combine the predictions of multiple models to produce a final result that is more accurate, more stable, and less likely to overfit than any single model. By using the strengths of many models together, ensemble methods improve overall performance and generalization."
      ],
      "metadata": {
        "id": "0P5A_F_FhWNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. What is the main challenge of ensemble methods ?\n",
        "   - The main challenge of ensemble methods is that they can become computationally expensive and harder to interpret because they involve training and combining many models instead of just one. This increases training time, memory usage, and makes the final decision-making process less transparent."
      ],
      "metadata": {
        "id": "SaIDi8FThhAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Explain the key idea behind ensemble techniques.\n",
        "- The key idea behind ensemble techniques is to combine the predictions of multiple models so that their collective decision is better than any single model alone. Different models capture different patterns and make different mistakes, and when they are combined, these errors tend to cancel out. As a result, ensemble methods improve accuracy, reduce overfitting, and create more reliable and stable predictions."
      ],
      "metadata": {
        "id": "zVvd34zPhtmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. What is a Random Forest Classifier ?\n",
        "- A Random Forest Classifier is an ensemble learning model that builds many decision trees on different random subsets of the data and features, and then combines their predictions to make the final decision. Each tree votes for a class, and the class with the most votes becomes the output. By using many diverse trees, a Random Forest reduces overfitting, improves accuracy, and provides a more stable and reliable classification compared to a single decision tree."
      ],
      "metadata": {
        "id": "Tos1Y9SwiDpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. What are the main types of ensemble techniques ?\n",
        "- The main types of ensemble techniques are:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating): Trains multiple models (usually of the same type) on different random subsets of the data (sampled with replacement). Predictions are combined by averaging (regression) or majority voting (classification). Aims to reduce variance and prevent overfitting. Example: Random Forest.\n",
        "\n",
        "2. Boosting: Trains models sequentially, where each new model tries to correct the errors of the previous ones. Focuses on data points that were difficult for prior models. Aims to reduce bias and convert weak learners into a strong one. Examples: AdaBoost, Gradient Boosting Machines (GBM), XGBoost.\n",
        "\n",
        "3. Stacking (Stacked Generalization): Trains several diverse base models and then uses a 'meta-learner' (another model) to learn how to optimally combine their predictions. Aims to leverage the strengths of different models for potentially better performance."
      ],
      "metadata": {
        "id": "7Rfb1irciQGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14.  What is ensemble learning in machine learning ?\n",
        "- Ensemble learning in machine learning is a powerful technique that combines the predictions of multiple individual models (often called 'base learners' or 'weak learners') to achieve a more accurate, robust, and generalizable prediction than any single model alone.\n",
        "The core idea is that by aggregating the 'collective wisdom' of several models, the ensemble can reduce errors, bias, and variance that individual models might exhibit. Different models often make different types of errors, and by combining them, these errors tend to cancel out."
      ],
      "metadata": {
        "id": "0QP_PHMPjB5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.  When should we avoid using ensemble methods ?\n",
        "- While ensemble methods are very powerful, there are situations where avoiding them or opting for simpler models might be more appropriate. Here are some key scenarios:\n",
        "\n",
        "1. Interpretability is Paramount: Ensemble models, especially complex ones like Random Forests or Gradient Boosting, are often considered 'black boxes.' It can be very challenging to understand why a specific prediction was made or to extract simple rules from them. If you need clear, transparent, and easily explainable models (e.g., for regulatory compliance, medical diagnostics, or client-facing models where trust is built on transparency), a simpler model like a linear regression, logistic regression, or a single decision tree might be preferred.\n",
        "\n",
        "2. Computational Resources (Time and Memory) are Limited: Ensemble methods involve training and storing multiple models, which can be significantly more computationally expensive and memory-intensive than training a single model. If you have severe constraints on training time, prediction latency, or available memory (e.g., edge devices, real-time systems, very large datasets with limited infrastructure), a simpler model might be a necessity.\n",
        "\n",
        "3. Very Small Datasets: For extremely small datasets, the benefits of ensemble methods (like variance reduction through bootstrapping) might not fully materialize, and they could even lead to overfitting if not carefully tuned. Simpler, more robust models might perform just as well or better and be easier to manage.\n",
        "\n",
        "4. No Significant Performance Improvement: If a simple, single model already achieves satisfactory performance and the additional complexity of an ensemble doesn't yield a meaningful improvement in accuracy or robustness, then the added complexity is unnecessary. Always consider the trade-off between performance gain and increased complexity.\n",
        "\n",
        "5. Data Quality Issues: If your data is very noisy, has a lot of irrelevant features, or significant outliers, ensemble methods might still struggle. While some ensembles are robust to noise, cleaning and preprocessing the data thoroughly should always precede complex modeling, rather than hoping an ensemble will magically fix data quality issues.\n",
        "\n",
        "6. Simplicity for Deployment/Maintenance: Deploying, monitoring, and maintaining multiple models in an ensemble can be more complex than managing a single model. If the operational overhead outweighs the performance gains, a simpler approach is better.\n",
        "\n",
        "7. Overfitting to Noise (in some cases): While ensembles generally reduce overfitting, poorly tuned boosting algorithms can sometimes be prone to overfitting the training data, especially if they are run for too many iterations. Careful cross-validation is always crucial"
      ],
      "metadata": {
        "id": "DDiaXMfVjZxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16. How does Bagging help in reducing overfitting ?\n",
        "- Bagging (Bootstrap Aggregating) is highly effective in reducing overfitting primarily through two mechanisms:\n",
        "\n",
        "1. Variance Reduction: Overfitting often stems from models that are too complex and capture noise or random fluctuations in the training data rather than the true underlying patterns. These models have high variance, meaning their predictions can change significantly with small changes in the training data.\n",
        "\n",
        "- How Bagging helps: Bagging trains multiple base models (e.g., decision trees) on different bootstrap samples of the training data. Because each sample is slightly different, each base model will learn slightly different patterns and might overfit to different aspects of its specific training subset. When the predictions from these diverse, overfitted base models are averaged (for regression) or combined by majority vote (for classification), the individual errors and the 'noise' captured by each model tend to cancel out. This averaging effect reduces the overall variance of the ensemble model significantly, leading to a more stable and generalized prediction.\n",
        "\n",
        "2. Increased Model Diversity: If all models were trained on the exact same data, they would likely make similar errors and exhibit similar overfitting. Combining them would not offer much benefit.\n",
        "\n",
        "- How Bagging helps: The bootstrap sampling ensures that each base model is trained on a slightly different subset of the original data. This introduces diversity among the base models, as they are exposed to different perspectives of the data. Some data points will be seen multiple times by some models, and not at all by others. This forces the individual models to learn different aspects of the underlying data distribution, making their individual biases and errors less correlated. When these diverse (and potentially individually overfitted) models are combined, their collective decision is less likely to suffer from the specific overfitting patterns of any single model."
      ],
      "metadata": {
        "id": "oriy1ltLkFZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17.  Why is Random Forest better than a single Decision Tree ?\n",
        "- Random Forest is better than a single Decision Tree because it builds many trees on different random subsets of data and features, and then combines their predictions. This reduces the chance of overfitting, which is a common problem with single decision trees. As a result, Random Forest provides more accurate, stable, and reliable predictions, and it generalizes better to new, unseen data"
      ],
      "metadata": {
        "id": "FqDq1ap2kskL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 18. What is the role of bootstrap sampling in Bagging ?\n",
        "-Bootstrap sampling is the foundational and most critical component of Bagging (Bootstrap Aggregating). Its role is multifaceted and essential for the technique's effectiveness in reducing variance and improving model stability. Here's a breakdown:\n",
        "\n",
        "1. Creating Diverse Training Subsets: The primary role of bootstrap sampling is to generate multiple, slightly different training datasets (bootstrap samples) from the original dataset. Each bootstrap sample is created by randomly drawing observations from the original data with replacement. This means an observation can be selected multiple times for a single bootstrap sample, and some observations might not be selected at all.\n",
        "\n",
        "2. Introducing Randomness and Diversity: By creating these varied samples, bootstrap sampling introduces randomness and diversity into the training process. Since each base model (e.g., decision tree) in the ensemble is trained on a different bootstrap sample, each model learns slightly different patterns and relationships from the data. This prevents all models from being identical and making the same errors.\n",
        "\n",
        "3. Enabling Variance Reduction: This diversity is key to Bagging's ability to reduce variance. When multiple diverse models, each trained on a slightly different view of the data, make predictions, and these predictions are then aggregated (averaged for regression, majority vote for classification), the individual errors, noise, and biases of each model tend to cancel each other out. This results in a more stable and less variable overall prediction from the ensemble.\n",
        "\n",
        "4. Facilitating Out-of-Bag (OOB) Evaluation: Bootstrap sampling also gives rise to the concept of Out-of-Bag (OOB) samples. For each base model, the observations from the original dataset that were not included in its particular bootstrap sample form its OOB set. These OOB samples can then be used to evaluate the performance of the model without needing a separate validation set. This provides an unbiased estimate of the generalization error that is \"free\" in terms of data usage and computation."
      ],
      "metadata": {
        "id": "gc2yOpIZk5id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19.  What are some real-world applications of ensemble techniques ?\n",
        "-  some real-world applications of ensemble techniques:\n",
        "\n",
        "1. Fraud Detection: Identifying fraudulent credit card transactions, insurance claims, or financial activities.\n",
        "2. Medical Diagnosis and Healthcare: Assisting in diagnosing diseases (e.g., cancer detection from medical images), predicting patient readmission, or identifying high-risk patients.\n",
        "3. Customer Churn Prediction: Predicting which customers are likely to cancel a subscription, switch providers, or stop using a service.\n",
        "4. Financial Market Prediction: Forecasting stock prices, bond yields, or market trends; algorithmic trading.\n",
        "5. Image and Speech Recognition: Applications in facial recognition, object detection in autonomous vehicles, speech-to-text systems, and natural language processing.\n",
        "6. Recommendation Systems: Recommending products on e-commerce sites, movies/shows on streaming platforms, or content on social media.\n",
        "7. Weather Forecasting: Predicting temperature, precipitation, wind speed, etc.\n",
        "8. Credit Scoring: Assessing the creditworthiness of loan applicants."
      ],
      "metadata": {
        "id": "WC6CHLtOlXVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 20. What is the difference between Bagging and Boosting?\n",
        "- Bagging\n",
        "Bagging trains many models independently and in parallel using different random subsets of the data. The final result is obtained by averaging (for regression) or voting (for classification). It mainly helps reduce variance and prevent overfitting.\n",
        "\n",
        "- Boosting\n",
        "Boosting trains models sequentially, where each new model focuses on fixing the mistakes of the previous one. Harder samples get higher importance. It mainly reduces bias and improves accuracy, but can overfit if not controlled."
      ],
      "metadata": {
        "id": "7qquklCemH2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21.  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy .\n"
      ],
      "metadata": {
        "id": "7a8A--9zmgcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "base_tree = DecisionTreeClassifier()\n",
        "bagging_model = BaggingClassifier(estimator=base_tree,n_estimators=50,random_state=1)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66B8iZ25nrlA",
        "outputId": "29c1a626-92f2-4c79-a472-fb395e78e630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22.  Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "GCK-2FnXoKUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5OxlPD4o9YY",
        "outputId": "744817ed-6791-411c-fd02-c813ee78b7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        " -  "
      ],
      "metadata": {
        "id": "_SCoLMm_pUtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "importances = rf_model.feature_importances_\n",
        "for name, score in zip(feature_names, importances):\n",
        "    print(f\"{name}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSQSXHO9pqu0",
        "outputId": "761d503e-b0c5-436e-f292-1833586d62d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean radius: 0.0323\n",
            "mean texture: 0.0111\n",
            "mean perimeter: 0.0601\n",
            "mean area: 0.0538\n",
            "mean smoothness: 0.0062\n",
            "mean compactness: 0.0092\n",
            "mean concavity: 0.0806\n",
            "mean concave points: 0.1419\n",
            "mean symmetry: 0.0033\n",
            "mean fractal dimension: 0.0031\n",
            "radius error: 0.0164\n",
            "texture error: 0.0032\n",
            "perimeter error: 0.0118\n",
            "area error: 0.0295\n",
            "smoothness error: 0.0059\n",
            "compactness error: 0.0046\n",
            "concavity error: 0.0058\n",
            "concave points error: 0.0034\n",
            "symmetry error: 0.0040\n",
            "fractal dimension error: 0.0071\n",
            "worst radius: 0.0780\n",
            "worst texture: 0.0188\n",
            "worst perimeter: 0.0743\n",
            "worst area: 0.1182\n",
            "worst smoothness: 0.0118\n",
            "worst compactness: 0.0175\n",
            "worst concavity: 0.0411\n",
            "worst concave points: 0.1271\n",
            "worst symmetry: 0.0129\n",
            "worst fractal dimension: 0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24.  Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "HEHLb3Z04Kx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Train a single Decision Tree Regressor\n",
        "tree_model = DecisionTreeRegressor(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_preds = tree_model.predict(X_test)\n",
        "# Train a Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "tree_mse = mean_squared_error(y_test, tree_preds)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "print(f\"Decision Tree MSE: {tree_mse:.3f}\")\n",
        "print(f\"Random Forest MSE: {rf_mse:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_rCCFbG4zAu",
        "outputId": "a02e3c51-ef4b-4d98-a70d-14c46b6a7203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree MSE: 0.528\n",
            "Random Forest MSE: 0.257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25.  Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n"
      ],
      "metadata": {
        "id": "lurFQClq5D6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", rf_model.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8iXsxo35M0m",
        "outputId": "91afc3e1-adb9-4f4f-aad9-5c0f65d7504b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.9547738693467337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#26.  Train a Bagging Classifier using SVM as a base estimator and print accuracy\n"
      ],
      "metadata": {
        "id": "3NNDx8a-5gdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
        "\n",
        "\n",
        "bagging_svm = BaggingClassifier(estimator=SVC(),n_estimators=20, random_state=2)\n",
        "\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier (SVM) Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z380lbyB5ln6",
        "outputId": "8af8bd45-c2f0-4da1-d46f-751822556c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier (SVM) Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#27. Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "l3ssxdJm57ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "tree_counts = [10, 50, 300, 1000]\n",
        "\n",
        "print(\"Number of Trees  |  Accuracy\")\n",
        "print(\"-----------------|-----------\")\n",
        "\n",
        "for n in tree_counts:\n",
        "    model = RandomForestClassifier(n_estimators=n, random_state=1)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{n:16} |  {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oi6yRL86PeW",
        "outputId": "89319c0b-ceac-441e-c210-2b6d125ac61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees  |  Accuracy\n",
            "-----------------|-----------\n",
            "              10 |  0.956\n",
            "              50 |  0.956\n",
            "             300 |  0.956\n",
            "            1000 |  0.956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#28. train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "ShZyp7Et6o0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging_lr = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=20000),\n",
        "    n_estimators=30,\n",
        "    random_state=42\n",
        "\n",
        ")\n",
        "\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "y_prob = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"Bagging Classifier (Logistic Regression) AUC Score:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s09CyHmE6oPd",
        "outputId": "3befe55c-3beb-435d-e849-f7ea15d2a46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier (Logistic Regression) AUC Score: 0.9977954144620811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#29. Train a Random Forest Regressor and analyze feature importance scores\n"
      ],
      "metadata": {
        "id": "985XQM6z7KnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "rf = RandomForestRegressor( n_estimators=200,random_state=1,n_jobs=-1)\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"Random Forest RMSE:\", rmse)\n",
        "\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": data.feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance Scores:\")\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H97N19Ug7SQq",
        "outputId": "144914a7-648b-4461-b95c-7a510dffb36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest RMSE: 0.5017423778887322\n",
            "\n",
            "Feature Importance Scores:\n",
            "      Feature  Importance\n",
            "0      MedInc    0.518076\n",
            "5    AveOccup    0.140813\n",
            "6    Latitude    0.094219\n",
            "7   Longitude    0.093108\n",
            "1    HouseAge    0.052613\n",
            "2    AveRooms    0.041136\n",
            "3   AveBedrms    0.030342\n",
            "4  Population    0.029693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#30.  Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "PTHK74FJ8LHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZiKjPd28jBk",
        "outputId": "38429a4e-a620-49c5-bb21-0e1521d2932c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.956140350877193\n",
            "Random Forest Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n"
      ],
      "metadata": {
        "id": "wQzP7s4E9mkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-qwyPFC9vua",
        "outputId": "028b2e78-af86-4b73-f218-5e03167aeb99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Test Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#32.Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "BjAlnnGS-Fku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "data =fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=4)\n",
        "estimators_list = [10, 50, 100, 200]\n",
        "results = {}\n",
        "for n in estimators_list:\n",
        "    model = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    results[n] = mse\n",
        "for n, mse in results.items():\n",
        "    print(f\"Estimators: {n}, MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9YbMHH--Jba",
        "outputId": "530c3181-143a-4f61-83db-6fa9bdb55537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimators: 10, MSE: 0.2798023544537357\n",
            "Estimators: 50, MSE: 0.254888592159683\n",
            "Estimators: 100, MSE: 0.254687797545832\n",
            "Estimators: 200, MSE: 0.2528488318343707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 33. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "VO1r0wsd_qo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n",
        "mis_idx = (y_pred != y_test)\n",
        "\n",
        "misclassified_samples = pd.DataFrame({\n",
        "    \"True Label\": y_test[mis_idx],\n",
        "    \"Predicted Label\": y_pred[mis_idx]\n",
        "})\n",
        "\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd6x6M-z_s2V",
        "outputId": "a6697daf-6d79-432c-8312-88eeecded0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956140350877193\n",
            "\n",
            "Misclassified Samples:\n",
            "   True Label  Predicted Label\n",
            "0           0                1\n",
            "1           0                1\n",
            "2           0                1\n",
            "3           0                1\n",
            "4           0                1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#34.  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "a67GzWWoAE6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaADnRzoAJMO",
        "outputId": "668864fc-becd-447a-c3a3-2449f98c4060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#35.  Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "3QmraWMmAqIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "pN4WcH4yAceP",
        "outputId": "45222995-0bf7-4e65-bc8d-dee14eab2507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "l_a_qpu8_MgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(kernel='rbf', probability=True, random_state=1)\n",
        "lr = LogisticRegression(max_iter=200)\n",
        "\n",
        "estimators = [\n",
        "    ('decision_tree', dt),\n",
        "    ('svm', svm)\n",
        "]\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=lr,\n",
        "    passthrough=False)\n",
        "\n",
        "stack_model.fit(X_train, y_train)\n",
        "y_pred = stack_model.predict(X_test)\n",
        "stack_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Stacking Classifier:\", stack_acc)\n",
        "dt.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "svm_acc = accuracy_score(y_test, svm.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"SVM Accuracy:\", svm_acc)\n",
        "print(\"Logistic Regression Accuracy:\", lr_acc)\n",
        "print(\"Stacking Classifier Accuracy:\", stack_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "uXJLJUaZAoKZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef100ea-9c87-42d6-a009-c14ec8f3233f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Stacking Classifier: 0.9666666666666667\n",
            "Decision Tree Accuracy: 0.9666666666666667\n",
            "SVM Accuracy: 0.9666666666666667\n",
            "Logistic Regression Accuracy: 0.9666666666666667\n",
            "Stacking Classifier Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#37. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "eWQH36Rz_syF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "importances = rf.feature_importances_\n",
        "feature_imp = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "top5 = feature_imp.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI07F3es_uzo",
        "outputId": "35c8b916-07ef-4768-dbb4-dc81add5f09b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "2  petal length (cm)    0.439994\n",
            "3   petal width (cm)    0.421522\n",
            "0  sepal length (cm)    0.108098\n",
            "1   sepal width (cm)    0.030387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#38.  Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "f_13Ni0iAPGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=1)\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "bagging_model = BaggingClassifier(estimator=base_estimator,n_estimators=50,random_state=1)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSjikTRcAYkK",
        "outputId": "21f017de-f48b-47d7-8667-912dcdaf9cac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9523809523809524\n",
            "Recall: 0.9743589743589745\n",
            "F1-score: 0.9610256410256411\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       0.86      1.00      0.92         6\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.95      0.97      0.96        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "uCT8UZArAzNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "depth_values = [None, 2, 3, 4, 5, 6, 8, 10]\n",
        "results = {}\n",
        "for depth in depth_values:\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=depth,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[depth] = acc\n",
        "print(\"Max Depth vs Accuracy:\")\n",
        "for depth, acc in results.items():\n",
        "    print(f\"max_depth = {depth}: accuracy = {acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TMzTsinA9XC",
        "outputId": "7b221413-2024-4572-9e92-a4bbc034fd5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth vs Accuracy:\n",
            "max_depth = None: accuracy = 1.0000\n",
            "max_depth = 2: accuracy = 1.0000\n",
            "max_depth = 3: accuracy = 1.0000\n",
            "max_depth = 4: accuracy = 1.0000\n",
            "max_depth = 5: accuracy = 1.0000\n",
            "max_depth = 6: accuracy = 1.0000\n",
            "max_depth = 8: accuracy = 1.0000\n",
            "max_depth = 10: accuracy = 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ],
      "metadata": {
        "id": "3f2LC3GJA6Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "data = load_diabetes()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "dt_base = DecisionTreeRegressor()\n",
        "bag_dt = BaggingRegressor(\n",
        "    estimator=dt_base,\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bag_dt.predict(X_test)\n",
        "dt_r2 = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "knn_base = KNeighborsRegressor()\n",
        "bag_knn = BaggingRegressor( estimator=knn_base, n_estimators=50, random_state=2)\n",
        "\n",
        "bag_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bag_knn.predict(X_test)\n",
        "knn_r2 = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "print(\"Bagging Regressor Performance:\")\n",
        "\n",
        "print(f\"Decision Tree Base R² Score : {dt_r2:.4f}\")\n",
        "print(f\"KNeighbors Base R² Score    : {knn_r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLBLSoinBRcp",
        "outputId": "ce33c11d-4aec-4a10-b0f6-8dbcaf512052"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor Performance:\n",
            "Decision Tree Base R² Score : 0.3292\n",
            "KNeighbors Base R² Score    : 0.3251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "xi1WeCr4A8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_bin, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "model = OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=1))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_prob = model.predict_proba(X_test)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob, average='macro')\n",
        "\n",
        "print(\"ROC-AUC Score (macro):\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s10F56HPB_IO",
        "outputId": "2482fd30-5df5-445a-bdcb-8f75c073f540"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score (macro): 0.9915472599296128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#42.Train a Bagging Classifier and evaluate its performance using cross-validation"
      ],
      "metadata": {
        "id": "MkYdCuSDCekD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "\n",
        "bag_model = BaggingClassifier(\n",
        "    estimator=base_estimator,\n",
        "    n_estimators=50,\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(bag_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", cv_scores.mean())\n",
        "print(\"Standard Deviation:\", cv_scores.std())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO4Vy3Z4CLy-",
        "outputId": "121c8959-fef9-4f86-a74a-d0407215b969"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy Scores: [0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n",
            "Mean Accuracy: 0.9666666666666668\n",
            "Standard Deviation: 0.02108185106778919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#43.Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "263XrxxJCuQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
        "y0 = y_bin[:, 0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y0, test_size=0.2, random_state=1)\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve (Class 0 vs Rest)\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gnXC-lu5C1FC",
        "outputId": "0e41aadc-a995-48c0-c015-0b4f50a2bcc4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "Mzdpi7YqDNFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=1)\n",
        "lr = LogisticRegression(max_iter=500)\n",
        "\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('lr', lr)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=500)\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "stack_acc = accuracy_score(y_test, stack_model.predict(X_test))\n",
        "\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n",
        "print(\"Logistic Regression Accuracy:\", lr_acc)\n",
        "print(\"Stacking Classifier Accuracy:\", stack_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_LWmBsvDOVw",
        "outputId": "cb47d21b-a1a4-4f70-d3eb-572868f6299c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9666666666666667\n",
            "Logistic Regression Accuracy: 0.9666666666666667\n",
            "Stacking Classifier Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "fx3Mjtb5DSkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "sample_sizes = [0.3, 0.5, 0.7, 1.0]\n",
        "results = {}\n",
        "\n",
        "for s in sample_sizes:\n",
        "    model = BaggingRegressor(estimator=DecisionTreeRegressor(),n_estimators=50, max_samples=s, bootstrap=True, random_state=1 )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    score = r2_score(y_test, y_pred)\n",
        "    results[f\"max_samples = {s}\"] = score\n",
        "\n",
        "for k, v in results.items():\n",
        "    print(k, \"=> R2 Score:\", round(v, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSB4vd6SDbh4",
        "outputId": "6013669b-793f-4fe8-cf4f-eb4d4141896f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_samples = 0.3 => R2 Score: 0.7893\n",
            "max_samples = 0.5 => R2 Score: 0.7983\n",
            "max_samples = 0.7 => R2 Score: 0.8005\n",
            "max_samples = 1.0 => R2 Score: 0.8047\n"
          ]
        }
      ]
    }
  ]
}
